{"cells":[{"cell_type":"markdown","id":"31708f12","metadata":{"id":"31708f12"},"source":["# Recurrent Neural Networks and Long Short Term Memory"]},{"cell_type":"markdown","id":"adf12abc","metadata":{"id":"adf12abc"},"source":["## 1. What is meant by Recurrent Neural Networks?"]},{"cell_type":"markdown","source":["- It is used for sequential data to solve common temporal problems seen in language translation and speech recognition etc\n","- It is a type of ANN which uses sequential data or time series data\n","- This algorithms are commonly used for ordinal or temporal problems, such as language translation, natural language processing (nlp), speech recognition, and image captioning\n","- they are popular applications such as Siri, voice search, and Google Translate\n","- Recurrent neural networks utilize training data to learn\n","- They take information from prior inputs to influence the current input and output\n","- While traditional deep neural networks assume that inputs and outputs are independent of each other, the output of recurrent neural networks depend on the prior elements within the sequence"],"metadata":{"id":"m-1jHHHitSaM"},"id":"m-1jHHHitSaM"},{"cell_type":"markdown","id":"20c2ff04","metadata":{"id":"20c2ff04"},"source":["## 2. What is meant by vanishing and exploding gradient and why is that a problem in RNN?"]},{"cell_type":"markdown","source":["- During backpropagation, we move backward through the network, calculating the derivative of the cost function J with respect to the weights in every layer\n","\n","# vanishing\n","- The vanishing gradient problem describes a situation encountered in the training of neural networks where the gradients used to update the weights shrink exponentially\n","- As a result, the weights are not updated anymore, and learning stops\n","- Hence the ANN is not trained efficiently to predict the outputs accurately\n","\n","# exploding gradient\n","- The exploding gradient problem describes a situation in the training of neural networks where the gradients used to update the weights grow exponentially\n","- This prevents the backpropagation algorithm from making reasonable updates to the weights, and learning becomes unstable.\n","- Hence the network is not efficient to predict the outputs accurately\n","\n","# Solutions\n","- Gradient Cliping\n","- Weight Initialization\n","- Use the ReLU Activation Function (non-saturating activation functions)\n","- Reducing the amount of Layers\n","\n","- RNN is a deep network and uses sigmoid funstions, in deep feedforward neural networks, backpropagation has \"the unstable gradient problem\".\n","- Training an RNN is a very difficult task\n","- It cannot process very long sequences if using tanh or relu as an activation function\n","\n","\n","\n"],"metadata":{"id":"kT1QmM_b12Fz"},"id":"kT1QmM_b12Fz"},{"cell_type":"markdown","id":"3e819185","metadata":{"id":"3e819185"},"source":["## 3. What is meant by Long Short Term Memory?"]},{"cell_type":"markdown","source":["- Long Short-Term Memory (LSTM) networks are a type of recurrent neural network\n","- They are capable of learning order dependence in sequence prediction problems\n","- This is a behavior required in complex problem domains like machine translation, speech recognition, and more.\n","- LSTMs are a complex area of deep learning\n","- requirements of a recurrent neural network\n","    - That the system be able to store information for an arbitrary duration\n","    - That the system be resistant to noise\n","    - That the system parameters be trainable"],"metadata":{"id":"ORc5Rntu-lrc"},"id":"ORc5Rntu-lrc"},{"cell_type":"markdown","id":"06c8e007","metadata":{"id":"06c8e007"},"source":["## 4. What is meant by Gated Recurrent Unit?"]},{"cell_type":"markdown","source":["- Gated recurrent unit is an advancement of the standard RNN\n","- GRUs are very similar to Long Short Term Memory(LSTM)\n","- Just like LSTM, GRU uses gates to control the flow of information\n","- They are relatively new as compared to LSTM\n","- This is the reason they offer some improvement over LSTM and have simpler architecture\n","- Another Interesting thing about  GRU is that, unlike LSTM, it does not have a separate cell state (Ct). It only has a hidden state(Ht). Due to the simpler architecture, GRUs are faster to train\n","- At each timestamp t, it takes an input Xt and the hidden state Ht-1 from the previous timestamp t-1. Later it outputs a new hidden state Ht which again passed to the next timestamp"],"metadata":{"id":"bUSb8eM__0pp"},"id":"bUSb8eM__0pp"},{"cell_type":"markdown","id":"d99da681","metadata":{"id":"d99da681"},"source":["## 5. Train a bi-directional LSTM on imdb movies sentiment dataset from keras (tutorial available on its website, follow that tutorial) (https://keras.io/examples/nlp/bidirectional_lstm_imdb/)"]},{"cell_type":"code","source":["import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","max_features = 30000  # Only consider the top 20k words\n","maxlen = 500  # Only consider the first 200 words of each movie review"],"metadata":{"id":"RP7PAJYHNHu9","executionInfo":{"status":"ok","timestamp":1656207873290,"user_tz":420,"elapsed":4168,"user":{"displayName":"Mehulkumar Kantaria","userId":"16147589357547038986"}}},"id":"RP7PAJYHNHu9","execution_count":1,"outputs":[]},{"cell_type":"code","source":["(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=max_features)\n","\n","print(len(x_train), \"Training sequences\")\n","print(len(x_val), \"Validation sequences\")\n","\n","x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E3gRRKK2B74q","executionInfo":{"status":"ok","timestamp":1656207878651,"user_tz":420,"elapsed":5366,"user":{"displayName":"Mehulkumar Kantaria","userId":"16147589357547038986"}},"outputId":"61070540-6fa2-4d22-b4ed-861ba1e4e6e0"},"id":"E3gRRKK2B74q","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n","17473536/17464789 [==============================] - 0s 0us/step\n","25000 Training sequences\n","25000 Validation sequences\n"]}]},{"cell_type":"code","source":["# Input for variable-length sequences of integers\n","inputs = keras.Input(shape=(None,), dtype=\"int32\")\n","\n","# Embed each integer in a 128-dimensional vector\n","x = layers.Embedding(max_features, 256)(inputs)\n","\n","# Add 2 bidirectional LSTMs\n","x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n","x = layers.Bidirectional(layers.LSTM(128))(x)\n","\n","# Add a classifier\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs, outputs)\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ST5tuH1PBAqR","executionInfo":{"status":"ok","timestamp":1656207883706,"user_tz":420,"elapsed":5060,"user":{"displayName":"Mehulkumar Kantaria","userId":"16147589357547038986"}},"outputId":"a4882822-2122-4eb9-8a56-e0c64b8d39c7"},"id":"ST5tuH1PBAqR","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," embedding (Embedding)       (None, None, 256)         7680000   \n","                                                                 \n"," bidirectional (Bidirectiona  (None, None, 256)        394240    \n"," l)                                                              \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 256)              394240    \n"," nal)                                                            \n","                                                                 \n"," dense (Dense)               (None, 1)                 257       \n","                                                                 \n","=================================================================\n","Total params: 8,468,737\n","Trainable params: 8,468,737\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n","model.fit(x_train, y_train, batch_size=32, epochs=50, validation_data=(x_val, y_val))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PMNJ0fP_COzo","outputId":"177831ab-4452-4c9c-9fea-66d3f720b8ba"},"id":"PMNJ0fP_COzo","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","782/782 [==============================] - 109s 126ms/step - loss: 0.4240 - accuracy: 0.8056 - val_loss: 0.3587 - val_accuracy: 0.8536\n","Epoch 2/50\n","782/782 [==============================] - 99s 126ms/step - loss: 0.2623 - accuracy: 0.8953 - val_loss: 0.3418 - val_accuracy: 0.8658\n","Epoch 3/50\n","782/782 [==============================] - 98s 126ms/step - loss: 0.1522 - accuracy: 0.9452 - val_loss: 0.3790 - val_accuracy: 0.8458\n","Epoch 4/50\n","392/782 [==============>...............] - ETA: 36s - loss: 0.0791 - accuracy: 0.9741"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"HW 12 SP22.ipynb","provenance":[],"collapsed_sections":[]},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}